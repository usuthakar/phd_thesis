

@Article{mr-com,
     AUTHOR    = "Mackey G, Sehrish S, Bent J",
     TITLE     = "Introducing map-reduce to high end computing. Petascale data storage",
     JOURNAL   = "PDSW",
     VOLUME    = "",
     YEAR      = "2008",
     PAGES     = "",
     doi       = "",
     eprint         = "2",
     archivePrefix  = "arXiv",
     primaryClass   = "hep-ex",
}
@Article{mr-data,
     AUTHOR    = "Ekanayake J, Pallickara S, Fox G.",
     TITLE     = "MapReduce for data intensive scientific analyses",
     JOURNAL   = "IEEE",
     VOLUME    = "",
     YEAR      = "2008",
     PAGES     = "",
     doi       = "",
     eprint         = "2",
     archivePrefix  = "arXiv",
     primaryClass   = "hep-ex",
}
@Article{intro-hdfs,
     AUTHOR    = "Attebury G, Baranovski A, et al.",
     TITLE     = "Hadoop distributed file system for the grid",
     JOURNAL   = "IEEE",
     VOLUME    = "",
     YEAR      = "2009",
     PAGES     = "",
     doi       = "",
     eprint         = "2",
     archivePrefix  = "arXiv",
     primaryClass   = "hep-ex",
}



@article{Verki1,
  title   ={{Big Data: Big Gaps of Knowledge in the Field of Internet Science}},
  journal = {International Journal of Internet Science},
  year    ={2012},
  volume  ={7, 1},
  pages   = {1--5},
  author  = {C. Snijders and U.  Matzat and U.-D  Reips}
}

@article{Aamnitchi2,
  title   ={{Filecules in highenergy physics: Characteristics and impact on resource management}},
  journal ={High Performance Distributed Computing},
  pages   ={69--80},
  year    ={2016},
  author  = {A. Aamnitchi and S. Doraimani and G. Garzoglio}
}

@book{Minoli3,
  author = {D. Minoli},
  title = {{A Networking Approach to Grid Computing}},
  publisher ={John Wiley & Sons, Inc.},
  year = {2015},
}


@article{Nicholson4,
  title   = {{Dynamic data replication in LCG 2008}},
  journal ={{Concurrency and Computation: Practice and Experience}},
  volume  ={20, 11},
  pages   = {1259--1271},
  year    ={2008},
  author  = {C. Nicholson and et al.}
}

@misc{LHC5,
  author = {CERN},
  title = {{LHC physics data taking gets underway at new record collision energy of 8TeV}},
  howpublished = "\url{ http://press.web.cern.ch}",
  year = {2012}, 
  note = "[Online; accessed 18-12-2015]"
}


@misc{Knobloch6,
  title   = {{LHC Computing Grid:Technical Design Report-LCG-TDR-001}},
  journal ={CERN},
  year    = {2015},
  author  = {J. Knobloch and L. Robertson}
}

@misc{LHCGrid7,
  author = {WLCG},
  title = {{Worldwide LHC Computing Grid}},
  howpublished = "\url{http://wlcg.web.cern.ch}",
  year = {2012}, 
  note = "[Online; accessed 20-12-2015]"
}

@misc{Grim8,
  title   ={{Tier-3 computing centers expand options for physicists}, { International Science Grid This Week (iSGTW)}},
  journal = { International Science Grid This Week (iSGTW)},
  year    = {2009},
  author  = {K. Grim}
}

@book{Foster9,
  author = {I. Foster and C. Kesselman},
  title = {{The Grid 2: Blueprint for a New Computing Infrastructure}},
  publisher = {Morgan Kaufmann Publishers Inc.},
  address ={San Francisco, CA, USA},
  year = {2013},
}

@article{Foster10,
  title   = {{The anatomy of the Grid: Enabling scalable virtual organizations}},
  journal ={The International Journal of High Performance Computing Applications},
  volume  ={15, 3},
  pages   ={200--222},
  year    = {2011},
  author  = {I. Foster and C. Kesselman and S. Tuecke}
}

@article{Laure11,
  title   ={{Programming the Grid with gLite}},
  journal = {Computational Methods in Science and Technology},
  volume  ={12, 1},
  pages   ={33--45},
  year    = {2006},
  author  = {E. Laure and {S.M Fisher} and A. Frohner and C. Grandi and {P.Z Kunszt} and A. Krenek and O. Mulmo and F. Pacini and F. Prelz and J. White and M. Barroso and P. Buncic and F. Hemmer and A. Di Meglio and A. Edlund}
}

@misc{Foster12,
  title   = {{What is the grid? A three point checklist, GRIDToday}},
  journal ={GRIDToday},
  year    = {2011},
  author  = {I. Foster}
}


@article{Dean13,
  title   = {{Mapreduce: Simplified data processing on large clusters}},
  journal = {Communications of the ACM},
  volume  ={51, 1},
  pages   = {107--113},
  year    = {2008},
  author  ={J. Dean and S. Ghemawat}
}

@article{Shvachko14,
  title   = {{The Hadoop Distributed File System}},
  journal ={IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)},
  pages   ={1--10},
  year    = {2010},
  author  ={K. Shvachko and H. Kuang and S. Radia and R. Chansler}
}

@article{Ghemawat15,
  title   = {{The google file system}},
  journal ={ACM SIGOPS Operating Systems Review},
  volume  ={37, 5},
  pages   = {29--43},
  year    ={2003},
  author  = {S. Ghemawat and H. Gobioff and {S.T Leung}}
}

@misc{Flume16,
  title = {{Apache Flume project}},
  howpublished = "\url{https://flume.apache.org}",
  year = {2012}, 
  note = "[Online; accessed 02-01-2016]"
}


@misc{Dirg17,
  author = {K. Skaburska},
  title = {{The Dirq project}},
  howpublished = "\url{http://dirq.readthedocs.org}",
  year = {2013}, 
  note = "[Online; accessed 27-12-2015]"
}


@book{Hadoop18,
  author = {T. White},
  title = {{Hadoop: The Definitive Guide, 3rd ed.}},
  publisher ={Yahoo press},
  year = {2012},
}


@article{Gardner19,
  author={R. Gardner and S. Campana and G. Duckeck and J. Elmsheuser and A. Hanushevsky and {F.G Hönig} and J. Iven and F. Legger and I. Vukotic and W. Yang and the Atlas Collaboration},
  title={{Data federation strategies for ATLAS using XRootD}},
  journal={Journal of Physics: Conference Series},
  volume={513},
  number={4},
  pages={042049},
  year={2014}
}

@article{Feynman198,
  title   ={{The theory of a general quantum system interacting with a linear dissipative system}},
  journal = {Annals of Physics},
  volume  = {24},
  pages   ={118--173},
  year    = {1963},
  doi     = {10.1016/0003-4916(63)90068-X},
  author  = {R.P Feynman and F.L {Vernon Jr.}}
}

@article{Magnoni20,
  author={L. Magnoni and U. Suthakar and C. Cordeiro and M. Georgiou and J. Andreeva and A. Khan and {D.R Smith}},
  title={{Monitoring WLCG with lambda-architecture: a new scalable data store and analytics platform for monitoring at petabyte scale}},
  journal={Journal of Physics: Conference Series},
  volume={664},
  number={5},
  pages={052023},
  year={2015}
}



@article{Andreeva2014,
	Abstract = {The Worldwide LHC Computing Grid (WLCG) today includes more than 150 computing centres where more than 2 million jobs are being executed daily and petabytes of data are transferred between sites. Monitoring the computing activities of the LHC experiments, over such a huge heterogeneous infrastructure, is extremely demanding in terms of computation, performance and reliability. Furthermore, the generated monitoring flow is constantly increasing, which represents another challenge for the monitoring systems. While existing solutions are traditionally based on Oracle for data storage and processing, recent developments evaluate NoSQL for processing large-scale monitoring datasets. NoSQL databases are getting increasingly popular for processing datasets at the terabyte and petabyte scale using commodity hardware. In this contribution, the integration of NoSQL data processing in the Experiment Dashboard framework is described along with first experiences of using this technology for monitoring the LHC computing activities.},
	Author = {Andreeva, J and Beche, A and Belov, S and Dzhunov, I and Kadochnikov, I and Karavakis, E and Saiz, P and Schovancova, J and Tuckett, D},
	Doi = {10.1088/1742-6596/513/3/032048},
	Issn = {17426596},
	Journal = {Journal of Physics: Conference Series},
	Pages = {32048},
	Title = {{Processing of the WLCG monitoring data using NoSQL}},
	Url = {http://stacks.iop.org/1742-6596/513/i=3/a=032048},
	Volume = {513},
	Year = {2014},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/513/i=3/a=032048},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/513/3/032048}}

@article{Andreeva2012,
	Abstract = {The automation of operations is essential to reduce manpower costs and improve the reliability of the system. The Site Status Board (SSB) is a framework which allows Virtual Organizations to monitor their computing activities at distributed sites and to evaluate site performance. The ATLAS experiment intensively uses the SSB for the distributed computing shifts, for estimating data processing and data transfer efficiencies at a particular site, and for implementing automatic exclusion of sites from computing activities, in case of potential problems. The ATLAS SSB provides a real-time aggregated monitoring view and keeps the history of the monitoring metrics. Based on this history, usability of a site from the perspective of ATLAS is calculated. The paper will describe how the SSB is integrated in the ATLAS operations and computing infrastructure and will cover implementation details of the ATLAS SSB sensors and alarm system, based on the information in the SSB. It will demonstrate the positive impact of the use of the SSB on the overall performance of ATLAS computing activities and will overview future plans.},
	Archiveprefix = {arXiv},
	Arxivid = {1301.0101},
	Author = {Andreeva, J and {Borrego Iglesias}, C and Campana, S and {Di Girolamo}, A and Dzhunov, I and {Espinal Curull}, X and Gayazov, S and Magradze, E and Nowotka, M M and Rinaldi, L and Saiz, P and Schovancova, J and Stewart, G A and Wright, M},
	Doi = {10.1088/1742-6596/396/3/032072},
	Eprint = {1301.0101},
	Issn = {1742-6588},
	Journal = {Journal of Physics: Conference Series},
	Number = {3},
	Pages = {032072},
	Title = {{Automating ATLAS Computing Operations using the Site Status Board}},
	Url = {http://arxiv.org/abs/1301.0101},
	Volume = {396},
	Year = {2012},
	Bdsk-Url-1 = {http://arxiv.org/abs/1301.0101},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/396/3/032072}}

@misc{Andreeva2010,
	Abstract = {During normal data taking CMS expects to support potentially as many as$\backslash$n2000 analysis users. Since the beginning of 2008 there have been more$\backslash$nthan 800 individuals who submitted a remote analysis job to the CMS$\backslash$ncomputing infrastructure. The bulk of these users will be supported at$\backslash$nthe over 40 CMS Tier-2 centres. Supporting a globally distributed$\backslash$ncommunity of users on a globally distributed set of computing clusters$\backslash$nis a task that requires reconsidering the normal methods of user support$\backslash$nfor Analysis Operations. In 2008 CMS formed an Analysis Support Task$\backslash$nForce in preparation for large-scale physics analysis activities. The$\backslash$ncharge of the task force was to evaluate the available support tools,$\backslash$nthe user support techniques, and the direct feedback of users with the$\backslash$ngoal of improving the success rate and user experience when utilizing$\backslash$nthe distributed computing environment. The task force determined the$\backslash$ntools needed to assess and reduce the number of non-zero exit code$\backslash$napplications submitted through the grid interfaces and worked with the$\backslash$nCMS experiment dashboard developers to obtain the necessary information$\backslash$nto quickly and proactively identify issues with user jobs and data sets$\backslash$nhosted at various sites. Results of the analysis group surveys were$\backslash$ncompiled. Reference platforms for testing and debugging problems were$\backslash$nestablished in various geographic regions. The task force also assessed$\backslash$nthe resources needed to make the transition to a permanent Analysis$\backslash$nOperations task. In this presentation the results of the task force will$\backslash$nbe discussed as well as the CMS Analysis Operations plans for the start$\backslash$nof data taking.},
	Author = {Andreeva, J and Calloni, M and Colling, D and Fanzago, F and D'Hondt, J and Klem, J and Maier, G and Letts, J and Maes, J and Padhi, S and Sarkar, S and Spiga, D and Mulders, P Van and Villella, I},
	Booktitle = {Journal of Physics: Conference Series},
	Doi = {10.1088/1742-6596/219/7/072007},
	Issn = {1742-6588},
	Number = {7},
	Pages = {072007},
	Title = {{CMS analysis operations}},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-6596/219/7/072007}}

@article{Andreeva2012a,
	Abstract = {Improvements in web browser performance and web standards compliance, as well as the availability of comprehensive JavaScript libraries, provides an opportunity to develop functionally rich yet intuitive web applications that allow users to access, render and analyse data in novel ways. However, the development of such large-scale JavaScript web applications presents new challenges, in particular with regard to code sustainability and team-based work. We present an approach that meets the challenges of large-scale JavaScript web application design and development, including client-side model-view-controller architecture, design patterns, and JavaScript libraries. Furthermore, we show how the approach leads naturally to the encapsulation of the data source as a web API, allowing applications to be easily ported to new data sources. The Experiment Dashboard framework is used for the development of applications for monitoring the distributed computing activities of virtual organisations on the Worldwide LHC Computing Grid. We demonstrate the benefits of the approach for large-scale JavaScript web applications in this context by examining the design of several Experiment Dashboard applications for data processing, data transfer and site status monitoring, and by showing how they have been ported for different virtual organisations and technologies.},
	Author = {Andreeva, J and Dzhunov, I and Karavakis, E and Kokoszkiewicz, L and Nowotka, M and Saiz, P and Tuckett, D},
	Doi = {10.1088/1742-6596/396/5/052069},
	File = {:home/edward/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreeva et al. - 2012 - Designing and developing portable large-scale JavaScript web applications within the Experiment Dashboard frame.pdf:pdf},
	Issn = {1742-6588},
	Journal = {Journal of Physics: Conference Series},
	Number = {5},
	Pages = {052069},
	Publisher = {IOP Publishing},
	Title = {{Designing and developing portable large-scale JavaScript web applications within the Experiment Dashboard framework}},
	Url = {http://iopscience.iop.org/1742-6596/396/5/052069},
	Volume = {396},
	Year = {2012},
	Bdsk-Url-1 = {http://iopscience.iop.org/1742-6596/396/5/052069},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/396/5/052069}}

@article{Andreeva2005,
	Abstract = { The CMS experiment is currently developing a computing system capable of serving, processing and archiving the large number of events that will be generated when the CMS detector starts taking data. During 2004 CMS undertook a large scale data challenge to demonstrate the ability of the CMS computing system to cope with a sustained data-taking rate equivalent to 25\% of startup rate. Its goals were: to run CMS event reconstruction at CERN for a sustained period at 25 Hz input rate; to distribute the data to several regional centers; and enable data access at those centers for analysis. Grid middleware was utilized to help complete all aspects of the challenge. To continue to provide scalable access from anywhere in the world to the data, CMS is developing a layer of software that uses Grid tools to gain access to data and resources, and that aims to provide physicists with a user friendly interface for submitting their analysis jobs. This paper describes the data challenge experience with Grid infrastructure and the current development of the CMS analysis system.},
	Author = {Andreeva, J. and Anjum, A. and Barrass, T. and Bonacorsi, D. and Bunn, J. and Capiluppi, P. and Corvo, M. and Darmenov, N. and {De Filippis}, N. and Donno, F. and Donvito, G. and Eulisse, G. and Fanfani, A. and Fanzago, F. and Filine, A. and Grandi, C. and Hern\'{a}ndez, J. M. and Innocente, V. and Jan, A. and Lacaprara, S. and Legrand, I. and Metson, S. and Newman, H. and Newbold, D. and Pierro, A. and Silvestris, L. and Steenberg, C. and Stockinger, H. and Taylor, L. and Thomas, M. and Tuura, L. and Wildish, T. and {Van Lingen}, F.},
	Doi = {10.1109/TNS.2005.852755},
	Isbn = {0018-9499},
	Issn = {00189499},
	Journal = {IEEE Transactions on Nuclear Science},
	Keywords = {Data flow analysis,Data management,Data processing,Distributed computing,Distributed information systems,High energy physics},
	Number = {4},
	Pages = {884--890},
	Title = {{Distributed computing grid experiences in CMS}},
	Volume = {52},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TNS.2005.852755}}

@article{Andreeva2004,
	Abstract = { In order to prepare the Physics Technical Design Report, due by end of 2005, the CMS experiment needs to simulate, reconstruct and analyse about 100 million events, corresponding to more than 200 TB of data. The data will be distributed to several Computing Centres. In order to provide access to the whole data sample to all the world-wide dispersed physicists, CMS is developing a layer of software that uses the Grid tools provided by the LCG project to gain access to data and resources and that aims to provide a user friendly interface to the physicists submitting the analysis jobs. To achieve these aims CMS will use Grid tools from both the LCG-2 release and those being developed in the framework of the ARDA project. This work describes the current status and the future developments of the CMS analysis system.},
	Author = {Andreeva, J. and Anjum, A. and Barrass, T. and Bonacorsi, D. and Bunn, J. and Corvo, M. and Darmenov, N. and Filippis, N. De and Donno, F. and Donvito, G. and Eulisse, G. and Fanfani, A. and Fanzago, F. and Filine, A. and Grandi, C. and Hernandez, J.M. and Innocente, V. and Jan, A. and Lacaprara, S. and Legrand, I. and Metson, S. and Newman, H. and Pierro, A. and Silvestris, L. and Steenberg, C. and Stockinger, H. and Taylor, L. and Thomas, M. and Tuura, L. and Wildish, T. and Lingen, F. Van},
	Doi = {10.1109/NSSMIC.2004.1462662},
	Isbn = {0-7803-8701-5},
	Issn = {1082-3654},
	Journal = {IEEE Symposium Conference Record Nuclear Science 2004.},
	Title = {{Use of grid tools to support CMS distributed analysis}},
	Volume = {4},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/NSSMIC.2004.1462662}}

@inproceedings{Andreeva2009,
	Abstract = {In this paper we present the Experiment Dashboard monitoring system, which is currently in use by$\backslash$r four Large Hadron Collider (LHC)[1] experiments. The goal of the Experiment Dashboard is to monitor$\backslash$r the activities of the LHC experiments on the distributed infrastructure, providing monitoring data$\backslash$r from the virtual organization (VO) and user perspectives. The LHC experiments are using various Grid$\backslash$r infrastructures (LCG[2]/EGEE[3], OSG[4], NDGF[5]) with correspondingly various middleware flavors$\backslash$r and job submission methods. Providing a uniform and complete view of various activities like job$\backslash$r processing, data movement and publishing, access to distributed databases regardless of the$\backslash$r underlying Grid flavor is the challenging task. In this paper we will describe the Experiment$\backslash$r Dashboard concept, its framework and main monitoring applications.},
	Author = {Andreeva, Julia and Gaidioz, Benjamin and Herrala, Juha and Maier, Gerhild and Rocha, Ricardo and Saiz, Pablo and Sidorova, Irina},
	Booktitle = {International Symposium on Grid Computing, ISGC 2007},
	Doi = {10.1007/978-0-387-78417-5-12},
	Isbn = {9780387784168},
	Issn = {1742-6596},
	Pages = {131--139},
	Title = {{Dashboard for the LHC experiments}},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-0-387-78417-5-12}}

@article{Dzhunov2014,
	Abstract = {Given the distributed nature of the Worldwide LHC Computing Grid and the way CPU resources are pledged and shared around the globe, Virtual Organizations (VOs) face the challenge of monitoring the use of these resources. For CMS and the operation of centralized workflows, the monitoring of how many production jobs are running and pending in the Glidein WMS production pools is very important. The Dashboard Site Status Board (SSB) provides a very flexible framework to collect, aggregate and visualize data. The CMS production monitoring team uses the SSB to define the metrics that have to be monitored and the alarms that have to be raised. During the integration of CMS production monitoring into the SSB, several enhancements to the core functionality of the SSB were required; They were implemented in a generic way, so that other VOs using the SSB can exploit them. Alongside these enhancements, there were a number of changes to the core of the SSB framework. This paper presents the details of the implementation and the advantages for current and future usage of the new features in SSB.},
	Author = {Dzhunov, I and Andreeva, J and Fajardo, E and Gutsche, O and Luyckx, S and Saiz, P},
	Doi = {10.1088/1742-6596/513/3/032028},
	Issn = {17426596},
	Journal = {Journal of Physics: Conference Series},
	Number = {3},
	Pages = {32028},
	Title = {{Towards a centralized Grid Speedometer}},
	Url = {http://stacks.iop.org/1742-6596/513/i=3/a=032028},
	Volume = {513},
	Year = {2014},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/513/i=3/a=032028},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/513/3/032028}}

@article{Girone2012,
	Abstract = {After two years of LHC data taking, processing and analysis and with numerous changes in computing technology, a number of aspects of the experiments' computing, as well as WLCG deployment and operations, need to evolve. As part of the activities of the Experiment Support group in CERN's IT department, and reinforced by effort from the EGI-InSPIRE project, we present work aimed at common solutions across all LHC experiments. Such solutions allow us not only to optimize development manpower but also offer lower long-term maintenance and support costs. The main areas cover Distributed Data Management, Data Analysis, Monitoring and the LCG Persistency Framework. Specific tools have been developed including the Hammer Cloud framework, automated services for data placement, data cleaning and data integrity (such as the data popularity service for CMS, the common Victor cleaning agent for ATLAS and CMS and tools for catalogue/storage consistency), the Dashboard Monitoring framework (job monitoring, data management monitoring, File Transfer monitoring) and the Site Status Board. This talk focuses primarily on the strategic aspects of providing such common solutions and how this relates to the overall goals of long-term sustainability and the relationship to the various WLCG Technical Evolution Groups.},
	Author = {Girone, M and Andreeva, J and Megino, F H Barreiro and Campana, S and Cinquilli, M and {Di Girolamo}, A and Dimou, M and Giordano, D and Karavakis, E and Kenyon, M J and Kokozkiewicz, L and Lanciotti, E and Litmaath, M and Magini, N and Negri, G and Roiser, S and Saiz, P and Santos, M D Saiz and Schovancova, J and Sciaba, A and Spiga, D and Trentadue, R and Tuckett, D and Valassi, A and {Van der Ster}, D C and Shiers, J D},
	Doi = {10.1088/1742-6596/396/3/032048},
	Issn = {1742-6588},
	Journal = {Journal of Physics Conference Series},
	Pages = {32048},
	Title = {{The "Common Solutions" Strategy of the Experiment Support group at CERN for the LHC Experiments}},
	Volume = {396},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-6596/396/3/032048}}

@article{Karavakis2014,
	Abstract = {This paper covers in detail a variety of accounting tools used to monitor the utilisation of the available computational and storage resources within the ATLAS Distributed Computing during the first three years of Large Hadron Collider data taking. The Experiment Dashboard provides a set of common accounting tools that combine monitoring information originating from many different information sources; either generic or ATLAS specific. This set of tools provides quality and scalable solutions that are flexible enough to support the constantly evolving requirements of the ATLAS user community.},
	Author = {Karavakis, E and Andreeva, J and Campana, S and Gayazov, S and Jezequel, S and Saiz, P and Sargsyan, L and Schovancova, J and Ueda, I and {the Atlas Collaboration}},
	Doi = {10.1088/1742-6596/513/6/062024},
	Issn = {17426596},
	Journal = {Journal of Physics: Conference Series},
	Pages = {62024},
	Title = {{Common Accounting System for Monitoring the ATLAS Distributed Computing Resources}},
	Url = {http://stacks.iop.org/1742-6596/513/i=6/a=062024},
	Volume = {513},
	Year = {2014},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/513/i=6/a=062024},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/513/6/062024}}

@misc{Karavakis2010,
	Abstract = {We are now in a phase change of the CMS experiment where people are turning more intensely to$\backslash$n physics analysis and away from construction. This brings a lot of challenging issues with respect to$\backslash$n monitoring of the user analysis. The physicists must be able to monitor the execution status,$\backslash$n application and grid-level messages of their tasks that may run at any site within the CMS Virtual$\backslash$n Organisation. The CMS Dashboard Task Monitoring project provides this information towards individual$\backslash$n analysis users by collecting and exposing a user-centric set of information regarding submitted$\backslash$n tasks including reason of failure, distribution by site and over time, consumed time and efficiency.$\backslash$n The development was user-driven with physicists invited to test the prototype in order to assemble$\backslash$n further requirements and identify weaknesses with the application.},
	Author = {Karavakis, Edward and Andreeva, Julia and Khan, Akram and Maier, Gerhild and Gaidioz, Benjamin},
	Booktitle = {Journal of Physics: Conference Series},
	Doi = {10.1088/1742-6596/219/7/072038},
	File = {:home/edward/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karavakis et al. - 2010 - CMS Dashboard Task Monitoring A user-centric monitoring view.html:html},
	Issn = {1742-6596},
	Number = {7},
	Pages = {072038},
	Title = {{CMS Dashboard Task Monitoring: A user-centric monitoring view}},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-6596/219/7/072038}}

@article{Megino2012,
	Abstract = {During the first two years of data taking, the CMS experiment has collected over 20 PetaBytes of data and processed and analyzed it on the distributed, multi-tiered computing infrastructure on the WorldWide LHC Computing Grid. Given the increasing data volume that has to be stored and efficiently analyzed, it is a challenge for several LHC experiments to optimize and automate the data placement strategies in order to fully profit of the available network and storage resources and to facilitate daily computing operations. Building on previous experience acquired by ATLAS, we have developed the CMS Popularity Service that tracks file accesses and user activity on the grid and will serve as the foundation for the evolution of their data placement. A fully automated, popularity-based site-cleaning agent has been deployed in order to scan Tier-2 sites that are reaching their space quota and suggest obsolete, unused data that can be safely deleted without disrupting analysis activity. Future work will be to demonstrate dynamic data placement functionality based on this popularity service and integrate it in the data and workload management systems: as a consequence the pre-placement of data will be minimized and additional replication of hot datasets will be requested automatically. This paper will give an insight into the development, validation and production process and will analyze how the framework has influenced resource optimization and daily operations in CMS.},
	Author = {Megino, F H Barreiro and Cinquilli, M and Giordano, D and Karavakis, E and Girone, M and Magini, N and Mancinelli, V and Spiga, D},
	Doi = {10.1088/1742-6596/396/3/032047},
	Issn = {17426588},
	Journal = {Journal of Physics: Conference Series},
	Pages = {32047},
	Title = {{Implementing data placement strategies for the CMS experiment based on a popularity model}},
	Url = {http://stacks.iop.org/1742-6596/396/i=3/a=032047},
	Volume = {396},
	Year = {2012},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/396/i=3/a=032047},
	Bdsk-Url-2 = {http://dx.doi.org/10.1088/1742-6596/396/3/032047}}

@article{Megino2012a,
	Abstract = {During the first two years of data taking, the CMS experiment has collected over 20 PetaBytes of data and processed and analyzed it on the distributed, multi-tiered computing infrastructure on the WorldWide LHC Computing Grid. Given the increasing data volume that has to be stored and efficiently analyzed, it is a challenge for several LHC experiments to optimize and automate the data placement strategies in order to fully profit of the available network and storage resources and to facilitate daily computing operations. Building on previous experience acquired by ATLAS, we have developed the CMS Popularity Service that tracks file accesses and user activity on the grid and will serve as the foundation for the evolution of their data placement. A fully automated, popularity-based site-cleaning agent has been deployed in order to scan Tier-2 sites that are reaching their space quota and suggest obsolete, unused data that can be safely deleted without disrupting analysis activity. Future work will be to demonstrate dynamic data placement functionality based on this popularity service and integrate it in the data and workload management systems: as a consequence the pre-placement of data will be minimized and additional replication of hot datasets will be requested automatically. This paper will give an insight into the development, validation and production process and will analyze how the framework has influenced resource optimization and daily operations in CMS.},
	Author = {Megino, F H Barreiro and Cinquilli, M and Giordano, D and Karavakis, E and Girone, M and Magini, N and Mancinelli, V and Spiga, D},
	Journal = {Journal of Physics: Conference Series},
	Pages = {32047},
	Title = {{Implementing data placement strategies for the CMS experiment based on a popularity model}},
	Url = {http://stacks.iop.org/1742-6596/396/i=3/a=032047},
	Volume = {396},
	Year = {2012},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/396/i=3/a=032047}}

@article{Munro2007,
	Abstract = {The Grid offers physicists far more resources than have previously been available by combining elements in computing centers around the world into a globally accessible resource. However, in order to harness this computing power many steps need to be performed for job creation and management and data discovery. ASAP (ARDA support for CMS analysis process) was developed to make this step as simple and straightforward as possible for physicists working on the Compact Muon Solenoid (CMS) experiment at CERN. ASAP transfers a local application to the Grid by packaging the application, discovering the location of input data then creating, submitting and monitoring jobs. One of the main components of the system is that users can delegate responsibility for their tasks to the ASAP server which can take actions on behalf of the user to ensure that tasks are completed successfully. The system can operate in the traditional push model but can also be used by agents executing on the Grid to pull jobs to the worker node for execution.},
	Author = {Munro, Craig and Andreeva, Julia and Khan, Akram},
	Doi = {10.1109/TNS.2007.905162},
	Issn = {00189499},
	Journal = {IEEE Transactions on Nuclear Science},
	Keywords = {Distributed analysis,Grid computing,High energy physics,Software},
	Number = {5},
	Pages = {1753--1757},
	Title = {{ASAP distributed analysis}},
	Volume = {54},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TNS.2007.905162}}

@inproceedings{Munro2007a,
	Abstract = {The Grid offers physicists far more resources than have previously available by combining elements in computing centers around the world into a globally accessible resource. In order to harness this available computing power many more steps need to be performed to package applications, locate data and control execution. The aim of the ASAP (ARDA Support for CMS Analysis Process) is to make this step as simple and straightforward for physicists on the CMS experiment at CERN. Starting from a local application and a configuration file ASAP can be used to transfer that application to the Grid by creating, submitting and monitoring jobs. One of the main components of this system is that users can delegate responsibility for their tasks to the ASAP Task Manager which can take actions on behalf of the user to ensure that tasks are completed successfully. Upwards of 30 physicists have used the system with 90,000 jobs submitted and 30 million events analysed every month.},
	Author = {Munro, Craig and Khan, Akram and Andreeva, Julia and Herrala, Juha and Kodolova, Olga},
	Booktitle = {IEEE Nuclear Science Symposium Conference Record},
	Doi = {10.1109/NSSMIC.2006.356229},
	Isbn = {1424405610},
	Issn = {10957863},
	Pages = {613--616},
	Title = {{Distributed analysis in ARDA/CMS}},
	Volume = {1},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/NSSMIC.2006.356229}}



@book{lambda,
  author = {Marz, Nathan and Warren,James},
  title = {Big Data. Principles and best practices of scalable realtime data systems.},
  isbn = {9781617290343},
  publisher = "Manning Publications",
  address = "",
  year = {2015},
}

@INPROCEEDINGS{mrgoogle,
    author = {Jeffrey Dean and Sanjay Ghemawat},
    booktitle = {MapReduce: simplified data processing on large clusters},
    year = {2004},
    publisher = {OSDI’04: Proceedings of the 6th conference on symposium on operating systems design and implementation - USENIX Association}
}

%% and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
 
@inproceedings{spark,
 author = "{Zaharia, Matei et all}",
 booktitle = {Resilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing},
 series = {NSDI'12},
 year = {2012},
 location = {San Jose, CA},
 pages = {2--2},
 numpages = {1},
%% url = {http://dl.acm.org/citation.cfm?id=2228298.2228301},
 acmid = {2228301},
 publisher = {Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation - USENIX Association},
 address = {Berkeley, CA, USA},
}

@UNPUBLISHED{esper,
  author = {Thomas Bernhardt},
  title = {The {E}sper project, http://www.espertech.com/esper},
  owner = {alb},
  timestamp = {2015.05.06}
}

%%     and Buncic, P and Carminati, F and Cattaneo, M and
%%                       Clarke, P and Fisk, I and Girone, M and Harvey, J and
                       Kersevan, B and Mato, P and Mount, R and Panzer-Steindel,
                       B",
 

@techreport{wlcg,
      author        = "{Bird, I et all.}", 
      title         = "{Update of the Computing Models of the WLCG and the LHC Experiments}",
      institution   = "CERN",
      address       = "Geneva",
      number        = "CERN-LHCC-2014-014. LCG-TDR-002",
      month         = "Apr",
      year          = "2014",
      reportNumber  = "CERN-LHCC-2014-014",
      url           = "https://cds.cern.ch/record/1695401",
}

@book{wlcg_old,
      author        = "Eck, Christoph and Knobloch, J and Robertson, Leslie and
                       Bird, I and Bos, K and Brook, N and Düllmann, D and Fisk,
                       I and Foster, D and Gibbard, B and Grandi, C and Grey, F
                       and Harvey, J and Heiss, A and Hemmer, F and Jarp, S and
                       Jones, R and Kelsey, D and Lamanna, M and Marten, H and
                       Mato-Vila, P and Ould-Saada, F and Panzer-Steindel, B and
                       Perini, L and Schutz, Y and Schwickerath, U and Shiers, J
                       and Wenaus, T",
      title         = "{LHC computing Grid: Technical Design Report. Version 1.06
                       (20 Jun 2005)}",
      publisher     = "CERN",
      address       = "Geneva",
      series        = "Technical Design Report LCG",
      year          = "2005",
      url           = "https://cds.cern.ch/record/840543",
}
 
